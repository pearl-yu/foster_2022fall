{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKe1vN1UrV3d"
      },
      "source": [
        "To open notebook in Colab please click below:\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/pearl-yu/foster_2022fall/blob/2022-master/Homeworks/HW%204.ipynb\" target=\"_parent\"> <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" /> </a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAMdF6y0oJLR"
      },
      "outputs": [],
      "source": [
        "#If opening in colab run this cell\n",
        "!git clone https://github.com/pearl-yu/foster_2022fall\n",
        "%cd foster_2022fall/Homeworks/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMTara0YoJLT"
      },
      "source": [
        "# IF OPENING IN COLAB, PLEASE REMEMBER TO SAVE THE NOTEBOOK TO YOUR GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkU2iycGoJLV"
      },
      "source": [
        "# Homework 4\n",
        "\n",
        "## Part I: Sentiment Analysis for Movie Reviews\n",
        "\n",
        "Sentiment analysis has become a common application of text data analytics, due to the immense amount of (user-generated) text data being created every day online.  Businesses can now look at what is being said about them online to get an idea of how well they are liked, how much they are disliked, and what they can do to improve.  While most of this data is unlabeled, some sites also ask users to provide a numerical or star rating.  This allows us to build a classifier for positive/negative reviews using the star rating as a label, which could then be applied to unlabeled text.\n",
        "\n",
        "IMDB collects information about movies and lets users write their own reviews, as well as provide a 1-10 numerical rating.   The data for this assignment can be found in 'IMDB_TRAIN.csv' and IMDB_TEST.csv', the training and test data respectively. It consists of thousands of positive and negative reviews collected from IMDB.  The ratings have been binarized by labeling anything with score between 7 and 10 as “P” and anything between 1 and 4 with “N” (there are no “neutral” reviews in the data set).  You will build and evaluate a system that classifies these movie reviews as positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ2UPKLQoJLW"
      },
      "source": [
        "__1. Load the training data and the test data. No points for this.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AR4ofA9goJLW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Remember to change this to the path of your files.\n",
        "train_path = 'data/IMDB_TRAIN.csv'\n",
        "test_path = 'data/IMDB_TEST.csv'\n",
        "\n",
        "columns = ['text', 'class']\n",
        "df_imdb_train = pd.read_csv(train_path, names=columns)\n",
        "df_imdb_test = pd.read_csv(test_path, names=columns)\n",
        "df_imdb_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkAzFoOYoJLX"
      },
      "source": [
        "__2. Build a Bernoulli Naive Bayes model to classify reviews according to their sentiment using a binary count vectorizer (check out the text mining module in the class notebooks if you do not know what this question is talking about). Test its accuracy and ROC AUC on the test data.  Is the accuracy good compared to just predicting a positive sentiment for all reviews?__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU8_XSUroJLY"
      },
      "outputs": [],
      "source": [
        "# Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmiyfOzEoJLY"
      },
      "source": [
        "__3. Pick a few reviews in the test set that were incorrectly classified and read them through.  Are there any words in these misclassified reviews that may have misled the classifier?  Explain with at least three examples for each type of error (i.e., false positive and false negative).__\n",
        "* For example: Instance number X contains the sentence, “Even her worst . . . I will stop here to avoid the spoiler.”  Even though this is actually positive, it was classified as a negative review; I would guess that this is due to the word “worst” in the review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OtZ0MUYoJLZ"
      },
      "outputs": [],
      "source": [
        "# Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE2N9iKDoJLZ"
      },
      "source": [
        "__4. Improve the accuracy of your model on the test data. Try at least two of the following to do this:__\n",
        "* __Use a non-binary count vectorizer or a tf-idf count vectorizer.__\n",
        "* __Use n-grams.__\n",
        "* __Remove stopwords.__\n",
        "\n",
        "__Try one other classifier besides Bernoulli Naive Bayes.__\n",
        "__Please report the accuracies__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wq7NWzLoJLa"
      },
      "outputs": [],
      "source": [
        "# Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnXeFXaEoJLa"
      },
      "source": [
        "## Part II: Multi-Class Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBegWBXPoJLa"
      },
      "source": [
        "There are lots of applications of text classification in the commercial world. For example, news stories are typically organized by topics; content or products are often tagged by categories; users can be classified into cohorts based on how they talk about a product or brand online. \n",
        "\n",
        "Suppose you've been hired by a finance institution that wants to classify incoming consumer complaints in product categories in order to forward them to the most appropriate customer support agents. You have a sample of the complaints that have been addressed in the past, each of them labeled as one category by the agent that addressed the complaint. Each complaint is assigned to one and only one category. \n",
        "\n",
        "Let's take a look at the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU9_tQq6oJLb"
      },
      "source": [
        "__1. Load the data. No points for this.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RttT-VEzoJLb"
      },
      "outputs": [],
      "source": [
        "# Remember to change this to the path of your file.\n",
        "complaints_path = 'data/complaints_sample.csv'\n",
        "\n",
        "data = pd.read_csv(complaints_path)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H7epMm_oJLb"
      },
      "source": [
        "__2. Print how many complaints there are for each product category.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5JlLNR_oJLb"
      },
      "outputs": [],
      "source": [
        "# Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EkZIZb6oJLb"
      },
      "source": [
        "__3. Split the data into a training and a test set. Build a logistic regression model to classify complaints according to product categories. Report its accuracy when applied to the test set. Why is this number relatively low compared to the binary accuracy in Part I? HINT: You may want to use [factorize](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html) to transform the target variable into a categorical variable. Take a look at the sample code below.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hpG-cKXoJLc"
      },
      "outputs": [],
      "source": [
        "# Example of how to transform a text variable into a categorical variable\n",
        "Y, cat_names = data['Product'].factorize() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uubKMQEMoJLc"
      },
      "outputs": [],
      "source": [
        "# Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqt8MtrcoJLc"
      },
      "source": [
        "__4. Make a confusion matrix that shows the errors made by your model. Use the test set labels and your predictions for the test set to build the matrix. Plot the confusion matrix using a Seaborn heatmap. What would perfect predictions look like? Give an example of one systematic type of error that you see. How can you tell that this is a systematic error? Why do you think such errors occur? HINT: Below there is code building and plotting a confusion matrix using the entire data and random predictions. The code removes the diagonal to highlight prediction errors.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOwCnVGMoJLc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "\n",
        "tick_labels = [c[:30] for c in cat_names]\n",
        "def plot_matrix(mat, title, remove_diagonal=True):\n",
        "    error_mat = mat.copy()\n",
        "    if remove_diagonal:\n",
        "        diag_ixs = np.arange(mat.shape[0])\n",
        "        error_mat[diag_ixs, diag_ixs] = 0\n",
        "    sns.heatmap(error_mat, annot=True, fmt='d', yticklabels=tick_labels, xticklabels=tick_labels)\n",
        "    plt.ylabel('Predictions')\n",
        "    plt.xlabel('True labels')\n",
        "    plt.title(title)\n",
        "\n",
        "random_predictions = np.random.choice(range(len(cat_names)), len(Y))\n",
        "random_conf_mat = confusion_matrix(random_predictions, Y)\n",
        "plot_matrix(random_conf_mat, 'Error Matrix', remove_diagonal=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "LojtGF8RoJLd"
      },
      "outputs": [],
      "source": [
        "# Your answer here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfy2Ru5OoJLd"
      },
      "source": [
        "__5. Suppose that the priority level of complaints varies by product category. You know that higher priority complaints need to be addressed faster. You also know that complaints incorrectly classified by the model will be given the correct classification (and therefore also the correct priority) by the first agent to which the complaint is assigned. If the agent has no knowledge about the product category of the complaint, he/she can forward the complaint to someone else. This is how priority level varies by product:__\n",
        "\n",
        "__High priority__\n",
        "* 'Money transfer, virtual currency, or money service'\n",
        "* 'Bank account or service'\n",
        "* 'Checking or savings account'\n",
        "\n",
        "__Normal priority__\n",
        "* 'Credit card or prepaid card'\n",
        "* 'Mortgage'\n",
        "* 'Student loan'\n",
        "* 'Consumer Loan'\n",
        "* 'Payday loan, title loan, or personal loan'\n",
        "* 'Vehicle loan or lease'\n",
        "\n",
        "__Low priority__\n",
        "* 'Credit reporting, credit repair services, or other personal consumer reports'\n",
        "* 'Debt collection'\n",
        "\n",
        "__Below you will find three cost matrices. Compute the total cost of your model according to each of them. If you were to compare multiple models, which cost matrix do you think would be better to compare the models? Why? Would you consider making any modifications to the cost matrix that you chose? If so, explain what would you consider changing and why. HINT: Below there is a sample code of how to compute the total cost of the random predictions.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41ptf0IRoJLd"
      },
      "outputs": [],
      "source": [
        "cost_matrix_1 = np.array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                          [1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                          [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                          [1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1],\n",
        "                          [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
        "                          [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1],\n",
        "                          [1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1],\n",
        "                          [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1],\n",
        "                          [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
        "                          [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
        "                          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
        "plot_matrix(cost_matrix_1, 'Cost Matrix 1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwiMNT5JoJLd"
      },
      "outputs": [],
      "source": [
        "cost_matrix_2 = np.array([[0, 2, 2, 2, 2, 1, 1, 2, 1, 1, 1],\n",
        "                          [2, 0, 3, 1, 3, 2, 2, 1, 2, 2, 2],\n",
        "                          [2, 3, 0, 3, 1, 2, 2, 3, 2, 2, 2],\n",
        "                          [2, 1, 3, 0, 3, 2, 2, 1, 2, 2, 2],\n",
        "                          [2, 3, 1, 3, 0, 2, 2, 3, 2, 2, 2],\n",
        "                          [1, 2, 2, 2, 2, 0, 1, 2, 1, 1, 1],\n",
        "                          [1, 2, 2, 2, 2, 1, 0, 2, 1, 1, 1],\n",
        "                          [2, 1, 3, 1, 3, 2, 2, 0, 2, 2, 2],\n",
        "                          [1, 2, 2, 2, 2, 1, 1, 2, 0, 1, 1],\n",
        "                          [1, 2, 2, 2, 2, 1, 1, 2, 1, 0, 1],\n",
        "                          [1, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0]])\n",
        "plot_matrix(cost_matrix_2, 'Cost Matrix 3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plhhdYjtoJLd"
      },
      "outputs": [],
      "source": [
        "cost_matrix_3 = np.array([[0, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1],\n",
        "                          [2, 0, 3, 1, 3, 2, 2, 1, 2, 2, 2],\n",
        "                          [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                          [2, 1, 3, 0, 3, 2, 2, 1, 2, 2, 2],\n",
        "                          [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
        "                          [1, 1, 2, 1, 2, 0, 1, 1, 1, 1, 1],\n",
        "                          [1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1],\n",
        "                          [2, 1, 3, 1, 3, 2, 2, 0, 2, 2, 2],\n",
        "                          [1, 1, 2, 1, 2, 1, 1, 1, 0, 1, 1],\n",
        "                          [1, 1, 2, 1, 2, 1, 1, 1, 1, 0, 1],\n",
        "                          [1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 0]])\n",
        "plot_matrix(cost_matrix_3, 'Cost Matrix 2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ-U-2HRoJLe"
      },
      "outputs": [],
      "source": [
        "# Replace random_conf_mat with the confusion matrix you generated in the last question.\n",
        "\n",
        "print(\"Cost with matrix 1: \", (random_conf_mat * cost_matrix_1).sum())\n",
        "print(\"Cost with matrix 2: \", (random_conf_mat * cost_matrix_2).sum())\n",
        "print(\"Cost with matrix 3: \", (random_conf_mat * cost_matrix_3).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7QYjMVDoJLe"
      },
      "outputs": [],
      "source": [
        "# Your answer here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "name": "HW 4.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}